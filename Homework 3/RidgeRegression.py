# -*- coding: utf-8 -*-
"""
Created on Wed Dec 30 10:23:58 2020

@author: jorge
"""

print(__doc__)

import numpy as np
import matplotlib.pyplot as plt

from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

                                                # Function that defines the system response
def f (vector, METHOD):
    if METHOD=="Sin":                   # Sin(x) function
        return np.sin(vector)
    else:                               # Default: x, i.e. Linear
        return vector
        

#system response

METHOD="Linear"                                    #"Linear" or "Sin"
        
                                                # Linear regression parameters
d=2                 # Different values of polynomial degree (from 0 to d-1)
Lambda=0        # Parameter for ridge regression. If set to 0 => ordinary lr

Lmax= 4*np.pi/5                                 # interval limit [-Lmax, Lmax]

                                                # training set parameters
m=5                                            # size of training set
Sigma2Epsilon=100                                 # variance of noise added to Y's

#iteration parameters

# We fix random values of X's, and for each configuration, we generate diffeent
# training sets by adding different realizations of noise

NsetsX=30       # Number of diferent sets of X's used to create training sets
NiterNoise=30   # Number of sets of Y's generated by adding noise added to Ytrue's per set X
Niter= NsetsX*NiterNoise # Number of different training sets used

# similar procedure for generating different testing sets
                                                # Testing set parameters
Ntesting=10                                     # Testing set size
y_test_true=np.zeros(Ntesting)                  # True values of Y's in testing sets
y_test_pred=np.zeros(shape=(Ntesting,Niter))    # Predicted values of Y's in testing sets


                                                # Stores random training points
X_grid = np.arange(-Lmax,Lmax,0.01)
X_grid = X_grid.reshape(len(X_grid),1) 

                                                # Stores random testing points
X_test=np.array([np.random.uniform(-Lmax,Lmax,Ntesting)])

                                                # True values of Y
y_test_true=f(X_test[0],METHOD)


                                                # stores values of Mean Square
                                                # Error, variance and Bias^2
                                                # for different polynomials degree
                                                # used in regression
MSE_tot=np.zeros(d)     
BIAS2_tot=np.zeros(d)
VAR_tot=np.zeros(d)
for k in range(1,d):    # degrees of polynomials in regression, avoid degree 0
    coefs=[]
    ridge=linear_model.Ridge(alpha=Lambda)   
    poly_reg = PolynomialFeatures(degree=k)
    X_test_poly=poly_reg.fit_transform(X_test.reshape(-1,1))
    MSE_iter=np.zeros(Niter)
    BIAS2_iter=np.zeros(Niter)
    VAR_iter=np.zeros(Niter)
    for j in range(NsetsX):
        X_train=np.random.uniform(-Lmax,Lmax,m) # Fix Niter values of X's at random positions in [-pi,pi]
        
        y=f(X_train,METHOD)                                # True values of Y
        X_poly = poly_reg.fit_transform(X_train.reshape(-1,1))  # Matrix X
                                               
                                                           #plot true values of Y
        plt.plot(X_grid, f(X_grid,METHOD),c="red")
        
        
        for i in range(NiterNoise):
            
            y_train=y+np.random.normal(0,Sigma2Epsilon,len(y))    # noisy values of Y's
    
            ridge.fit(X_poly,y_train)                       # linear regression

            coefs.append(ridge.coef_[1])                    # coefficient theta_1
            
                                                            # predictyed values of Y
            y_test_pred[:,j*NiterNoise+i]=ridge.predict(X_test_poly)
            
                                                            #Mean squared error for this training/testing set
            MSE_iter[j*NiterNoise+i]=mean_squared_error(y_test_true,y_test_pred[:,j*NiterNoise+i])
            
                                                            #plot results:
            plt.title("EXAMPLES of Ridge regression, lambda:"+str(Lambda)+" polynomial of degree:"+str(k))
            plt.xlabel('Value of X')
            plt.ylabel('Value of Y')
            
            if ( (j < 10) and (i==0)):   
                #plt.scatter(X_train,y, color='red')             #plot training set
                #plt.scatter(X_train, y_train,c="green")
                #plt.plot(X_grid, X_grid,c="red")
                                                            #plot polynomial fit
                plt.plot(X_grid, ridge.predict(poly_reg.fit_transform(X_grid)),color='gray')
                
                #plt.show()
                #plt.scatter(X_test, y_test_pred[:,j*NiterNoise+i],c="orange")
                #plt.scatter(X_test, y_test_true,c="orange")
            
            
            
        
    plt.show()
    plt.title("Histogram, coefficient theta_1")
    plt.hist(coefs,density=True, range=(-5,5), bins=30)
    plt.show()

    print("====== Bias and variance for coefficient theta_1") 
    print("mean value of coefficient theta_1:",np.mean(coefs))
    print("variance of coefficient theta_1:",np.var(coefs))
    MSE_tot[k]=np.mean(MSE_iter)
    BIAS2_tot[k]=np.mean((np.mean(y_test_pred,axis=1)-y_test_true)**2)
    VAR_tot[k]=np.mean(np.var(y_test_pred,axis=1))

print("")    
print("====== MSE, Bias^2 and Variance for y_p - y  for different polynomials")    

if d>2 :
    plt.title("Mean Sqare Error (black), Bias^2 (red), Variance (green)")
    plt.xlabel('Polynomial degree')
    plt.ylabel('MSE')
    plt.plot(np.arange(1,d,1), MSE_tot[1:d],color='black')
    plt.plot(np.arange(1,d,1), BIAS2_tot[1:d],color='red')
    plt.plot(np.arange(1,d,1), VAR_tot[1:d],color='green')
    plt.show()
else:
    print("polynomial of degree:",d-1)
    print("Mean Square Error:",MSE_tot[1])
    print("Bias^2:",BIAS2_tot[1])
    print("Variance:",VAR_tot[1])

  




